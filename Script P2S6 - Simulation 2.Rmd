---
title: "Simulation Results"
output:
  html_document:
    df_print: paged
---

Draft Version Nov 2020. 

# Introduction

We compared different approaches to estimating canonical correlations. 

For a concise guide to terminology, see^[https://www.stata.com/manuals/mvcanon.pdf].

A key issue with canonical correlation analysis is model overfit. 

With large numbers of variables and few participants, in-sample estimates of canonical correlation strength may be very large - even when there's no underlying relationship between predictor and outcome variables.

We tested two approaches to generating unbiased canonical correlations. 

### Approach 1 - Cross-Validation-Bootstrap 

First, we evaluated a Cross-Validation-Bootstrap approach to model fitting, which runs using the *gb_CCA_CVboot* function. This approach involves fitting a N-fold (default is 10-fold) CCA model, and then bootstrapping the cross-validated canonical variates, which are then used to estimate the ("predicted" or "cross-validated") canonical correlations. P-values can then be estimated from the bootstrap confidence interval via The Test-Inversion Lemma (i.e. a 1 - $\alpha$ % confidence interval does not contain the null can be considered significant for a test of size $\alpha$). 

Whilst this approach is less intuitive than performing model-fitting on various bootstrap-resampled datasets, bootstrap resampling inherently leads to having many (on average 36.8%) duplicate observations, which leads to overfitting even with cross-validation. 

One issue with cross-validation of CCA models is that running multiple  CCA models on different datasets, the sign or order of canonical variates can change. 

For example, imagine a PCA is run on a cognitive ability battery in multiple samples from a larger population. In some cases, the first component (PC1) may have stronger weights on verbal ability items, and PC2 will have larger weights or non-verbal ability items. In other cases,  PC1 and PC2 may swap, so that the first component represents non-verbal ability items to a stronger extent. It is also possible to get positive and negative canonical correlations where the eigenvector for one canoncical variate have all changed sign, but have roughly the same magnitude.

In the *gb_CCA_CVboot* function (when gb_CCA_CVboot(..., UseProcrustes=TRUE)), we first run an "initial" CCA model in the complete dataset, and when future models are fitted in each training fold, the estimated canonical variate weights (eigenvectors) are rotated using a procrustes rotation (a special rotation which can only utilising dilation, rotation and reflection) to map the CCA models from training folds to the "initial" CCA model weights. 

However, this approach may lead to bias in the estimated canonical correlations, as data from the whole sample is indirectly used within each training fold. 

### Approach 2 - Simple Training/Testing Split

Secondly, we evaluated a simpler approach where the CCA model is fitted in a training dataset, and the raw coefficients are used to estimate *predicted* canonical variates in the testing dataset. 

Because canonical variates in the testing dataset are simply linear combinations of raw variables, standard correlations between variables can be estimated, and standard inferential methods for correlations used.

The function *gb_CCA_SplitHalf* automates model fitting for simulations by automatically splitting the input data in half, and then estimating Pearson correlations between "predicted" canonical variates. The function also estimates confidence intervals (using the Fisher transformation approach), and p-values (using Student's t-distribution) for Pearson's correlation coefficient. 

### Data Simulation

To evaluate both methods estimation methods were run on simulated data (generated by *SimScript2.R*).

4 large datasets (N = 1,000,000) were created simulating 30 predictor and 30 outcome variables with only a single canonical correlation of varying strength linking the two sets of variables. 

First, two unmeasured *latent* variable (x,y) scores were generated for each simulated "participant" (i.e. 1,000,000 latent variable scores for x & y), where the correlation between latent variables was manipulated to be .5, .3, .1, or 0 for datasets 4, 3, 2, and 1 respectively. 

To generate the 30 *observed* predictor and outcome variables that were used in analyses, each observed variable $(X,Y)$ is a sum of latent score (x or y) multiplied by a variable-specific weight (w) plus a random error term ($wX + e$). Errors were generated using a normal distribution (mean = 0, sd = 1) random number generator, and weights for the 60 observed variables were generated from a uniform distribution ( 0 and 1.2 as lower and upper limits).

# Approach 1 Evaluation

This approach is computationally costly to run repeatedly on simulated data. Whilst the analyses were run using a Linux cluster (using *SimScript3.R*),  we could not make use of the more powerful "compute nodes" as a dependency (GSL) was not installed, which limited the number of repeats that could be performed. 

Simulations varied the canonical correlation strength (r = 0.0, 0.1, 0.3, 0.5) and sample size drawn (300, 5000, 10000). For each combination of these parameters, 150 repeated analyses were performed.


```{r}
rm(list=ls())
knitr::opts_chunk$set(cache = TRUE) #Cache's results for quick fitting (or not, if false)! 
load("Output R Data/ClusterSimData_PROCRUSTES_2020_11_03_04_05_26.Rdata")
ClusterSim1 = OutputData # With procrustes 
rm(OutputData)

library(ggplot2)
library(patchwork)

```



```{r, fig.height=9, fig.width=10}

ClusterSim1_Pvalues = lapply(names(ClusterSim1), function(x)
                        lapply(names(ClusterSim1[[1]]), function(y)
                          lapply(1:length(ClusterSim1[[1]][[1]]), function(z)
                            (ClusterSim1[[x]][[y]][[z]]$CrossValidationBootstrapPvalues)
                          )))

ClusterSim1_Pvalues = data.frame(reshape2::melt(ClusterSim1_Pvalues))
  colnames(ClusterSim1_Pvalues) = c("p","rep", "SampleSize", "Dataset")
  ClusterSim1_Pvalues$SampleSize = dplyr::recode(ClusterSim1_Pvalues$SampleSize, `1`=300, `2`=5000,`3`=10000)
  ClusterSim1_Pvalues$CanonicalCorrelation = rep(1:5, 300)
  ClusterSim1_Pvalues$Sig = as.numeric(ClusterSim1_Pvalues$p<=0.05)
  
  
  library(dplyr)  
  ClusterSim1_Pvalues2 = ClusterSim1_Pvalues %>% group_by(CanonicalCorrelation,SampleSize,Dataset) %>% summarise(SigRate = mean(Sig))
  ClusterSim1_Pvalues2$SampleSize = as.factor(ClusterSim1_Pvalues2$SampleSize)


p1=
ggplot(ClusterSim1_Pvalues2[ClusterSim1_Pvalues2$Dataset==1,], aes(x=CanonicalCorrelation,y=SigRate, group=SampleSize, col=SampleSize)) + geom_line() + geom_point() +
  jtools::theme_apa() + geom_hline(yintercept = 0.05, linetype="dashed") + 
  labs(title="Dataset 1 (r=0.5)",y="Proportion Significant (100 repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black"))

p2=
ggplot(ClusterSim1_Pvalues2[ClusterSim1_Pvalues2$Dataset==2,], aes(x=CanonicalCorrelation,y=SigRate, group=SampleSize, col=SampleSize)) + geom_line() + geom_point() +
  jtools::theme_apa() + geom_hline(yintercept = 0.05, linetype="dashed") + 
  labs(title="Dataset 2 (r=0.3)",y="Proportion Significant (100 repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black"))

p3=
ggplot(ClusterSim1_Pvalues2[ClusterSim1_Pvalues2$Dataset==3,], aes(x=CanonicalCorrelation,y=SigRate, group=SampleSize, col=SampleSize)) + geom_line() + geom_point() +
  jtools::theme_apa() + geom_hline(yintercept = 0.05, linetype="dashed") + 
  labs(title="Dataset 3 (r=0.1)",y="Proportion Significant (100 repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black"))

p4=
ggplot(ClusterSim1_Pvalues2[ClusterSim1_Pvalues2$Dataset==4,], aes(x=CanonicalCorrelation,y=SigRate, group=SampleSize, col=SampleSize)) + geom_line() + geom_point() +
  jtools::theme_apa() + geom_hline(yintercept = 0.05, linetype="dashed") + 
  labs(title="Dataset 4 (r=0)",y="Proportion Significant (100 repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black"))

p1 + p2 + p3 + p4 + plot_layout(ncol=2, byrow = TRUE) + plot_annotation(title = 'Figure S1', subtitle = 'Percentage of significant canonical correlations when drawing repeated (150 repeated) samples from simulated data.  Datasets were simulated with only a single canonical correlation, of varying strength (r=0.5-0.0).')

```

Figure S1 displays the percentage of significant (p < 0.05) p-values across each condition. Each point represents an average value over 150 repetitions. 

What we should observe, is for the first canonical correlation, when the simulated canonical correlation is greater than 0, a high proportion should be significant. The proportion of significant canonical correlations in these cases would represent statistical power, that is, the probability of finding a significant effect given some non-null population effect exists.  

All additional canonical corrleations (2-5) should only be significant 5% of the time, as we chose a 5% alpha level to define significance. This is because, when the null hypothesis is true, the false-positive error rate should be controlled at the alpha level (5%).

Instead, we observed a high percentage of significant p-values for the 2nd, 3th and 5th canonical correlations across all sample sizes. 

It is likely that by rotating the raw coefficients to match the full-sample raw coefficients, it is inducing some bias.


```{r, fig.height=9, fig.width=10,  echo = FALSE }


ClusterSim1_BCV = lapply(names(ClusterSim1), function(x)
                        lapply(names(ClusterSim1[[1]]), function(y)
                          lapply(1:length(ClusterSim1[[1]][[1]]), function(z)
                            (ClusterSim1[[x]][[y]][[z]]$CrossValidationBootstrapQuantiles)
                          )))

ClusterSim1_BCV2 = reshape2::melt(ClusterSim1_BCV)
  colnames(ClusterSim1_BCV2) = c("percentile", "CanonicalCorrelation","value","rep","SampleSize","Dataset")
  
ClusterSim1_BCV_Avgd = ClusterSim1_BCV2 %>% 
                        group_by(CanonicalCorrelation,SampleSize,Dataset) %>% 
                         summarise(MeanR2 = mean(value^2))
ClusterSim1_BCV_Avgd$SampleSize = factor(dplyr::recode(ClusterSim1_BCV_Avgd$SampleSize, `1`=300, `2`=5000,`3`=10000))
  
  
ClusterSim1_BCV_NoAvgd = tidyr::pivot_wider(ClusterSim1_BCV2, names_from = percentile, values_from = value)
  # colnames(ClusterSim1_BCV3) = c("CanonicalCorrelation","rep", "SampleSize", "Dataset","LB","MEDIAN","UB")
  # ClusterSim1_BCV3$SampleSize = dplyr::recode(ClusterSim1_BCV3$SampleSize, `1`=300, `2`=5000,`3`=10000)
  # ClusterSim1_BCV3$CanonicalCorrelation = as.factor(ClusterSim1_BCV3$CanonicalCorrelation)
  # 
  
# ClusterSim1_BCV3 = ClusterSim1_BCV3 %>% group_by(CanonicalCorrelation,SampleSize,Dataset) %>% summarise(SigRate = mean(Sig))


p1=
ggplot(ClusterSim1_BCV_Avgd[ClusterSim1_BCV_Avgd$Dataset==1,], aes(x=CanonicalCorrelation,y=MeanR2, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = c(.5^2), linetype="dashed") + geom_hline(yintercept = c(0.472649666^2), linetype="dashed", col="grey") + 
  labs(title="Dataset 1 (r=0.5)",y="Average R2 (100 Repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,.25))

p2=
ggplot(ClusterSim1_BCV_Avgd[ClusterSim1_BCV_Avgd$Dataset==2,], aes(x=CanonicalCorrelation,y=MeanR2, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = .3^2, linetype="dashed") + geom_hline(yintercept = c(0.276494574^2), linetype="dashed", col="grey") + 
  labs(title="Dataset 2 (r=0.3)",y="Average R2 (100 Repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,.25))

p3=  
ggplot(ClusterSim1_BCV_Avgd[ClusterSim1_BCV_Avgd$Dataset==3,], aes(x=CanonicalCorrelation,y=MeanR2, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = .1^2, linetype="dashed") + geom_hline(yintercept = c(0.092387498^2), linetype="dashed", col="grey") + 
  labs(title="Dataset 3 (r=0.1)",y="Average R2 (100 Repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,.25))

p4=
ggplot(ClusterSim1_BCV_Avgd[ClusterSim1_BCV_Avgd$Dataset==4,], aes(x=CanonicalCorrelation,y=MeanR2, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + 
  geom_hline(yintercept = 0, linetype="dashed") +
  labs(title="Dataset 4 (r=0)",y="Average R2 (100 Repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,.25))

p1 + p2 + p3 + p4 + plot_layout(ncol=2, byrow = TRUE) + plot_annotation(title = 'Figure S2', subtitle = 'Average Canonical Correlation Strength (R2) from repeated sampling of the simulated datasets')


```


Figure S2 displays the average squared canonical correlations across the same simulations. These are squared because canonical correlations can be negative (e.g. r = -.49 in Dataset 1) because the predictor or outcome raw coefficientshave changed sign. As one paper puts it more eloquently: "the signs of the canonical variables and coefficients are indeterminate, and any solution is arbitrary; nothing can be concluded about the specific direction of effects with CCA"^[https://doi.org/10.1016/j.neuroimage.2020.117065]. 

With large (N > 300) sample sizes, we get sensible R2 estimates. The first canonical correlations are large and similar to the simulated canonical correlations, and subsequent canonical correlations ones are very close to 0. 

With small samples (N = 300), average squared canonical correlations appear to be relatively large across the board. 



```{r, fig.height=9, fig.width=10, echo = FALSE }

p1=
ggplot(data=ClusterSim1_BCV_NoAvgd[(ClusterSim1_BCV_NoAvgd$Dataset==4 & ClusterSim1_BCV_NoAvgd$SampleSize==3) & ClusterSim1_BCV_NoAvgd$CanonicalCorrelation=="cc1" ,], aes_string(x="rep",y="`50%`")) + 
  geom_point() + geom_errorbar(aes_string(ymin="`2.5%`",ymax="`97.5%`"))  + 
  jtools::theme_apa() + 
  labs(y="Estimated Effects and Cofidence Intervals", x="Iteration", title= "1st Canonical Correlation") + 
  coord_cartesian(ylim = c(-.09, .09))

p2=
ggplot(data=ClusterSim1_BCV_NoAvgd[(ClusterSim1_BCV_NoAvgd$Dataset==4 & ClusterSim1_BCV_NoAvgd$SampleSize==3) & ClusterSim1_BCV_NoAvgd$CanonicalCorrelation=="cc2" ,], aes_string(x="rep",y="`50%`")) + 
  geom_point() + geom_errorbar(aes_string(ymin="`2.5%`",ymax="`97.5%`"))  + 
  jtools::theme_apa() + 
  labs(y="Estimated Effects and Cofidence Intervals", x="Iteration", title= "2nd Canonical Correlation")+ 
  coord_cartesian(ylim = c(-.09, .09))

p3=
ggplot(data=ClusterSim1_BCV_NoAvgd[(ClusterSim1_BCV_NoAvgd$Dataset==4 & ClusterSim1_BCV_NoAvgd$SampleSize==3) & ClusterSim1_BCV_NoAvgd$CanonicalCorrelation=="cc3" ,], aes_string(x="rep",y="`50%`")) + 
  geom_point() + geom_errorbar(aes_string(ymin="`2.5%`",ymax="`97.5%`"))  + 
  jtools::theme_apa() + 
  labs(y="Estimated Effects and Cofidence Intervals", x="Iteration", title= "3rd Canonical Correlation")+ 
  coord_cartesian(ylim = c(-.09, .09))

p4=
ggplot(data=ClusterSim1_BCV_NoAvgd[(ClusterSim1_BCV_NoAvgd$Dataset==4 & ClusterSim1_BCV_NoAvgd$SampleSize==3) & ClusterSim1_BCV_NoAvgd$CanonicalCorrelation=="cc4" ,], aes_string(x="rep",y="`50%`")) + 
  geom_point() + geom_errorbar(aes_string(ymin="`2.5%`",ymax="`97.5%`"))  + 
  jtools::theme_apa() + 
  labs(y="Estimated Effects and Cofidence Intervals", x="Iteration", title= "4th Canonical Correlation")+ 
  coord_cartesian(ylim = c(-.09, .09))



p1 + p2 + p3 + p4 + plot_layout(ncol=2, byrow = TRUE) + plot_annotation(title = 'Figure S3 - Dataset 4 only (r=0)', subtitle = 'Estimated Canonical Correlations and 95% Cross-Validated Boostrap Confidence Intervals.\nDespite the simulated canonical correlation being 0, we observe a small bias, which means that confidence intervals often do not overap with 0.')


```

Figure S3 plots the estimated confidence intervals for each repeated simulation, for analyses of simulated dataset 4 (0 population canonical correlation) and sample sizes of 10,000 are drawn. 

The plots show a small but very consistent bias mostly clearly evident for the first canonical correlation. Whilst the estimated canonical correlations are very small and near 0, they are bimodally distributed around about .03 and -.03, and confidence intervals have poor coverage. 


# Evaluating Split-Half Validation Approach 

```{r, fig.height=9, fig.width=10, echo = FALSE }
load("Output R Data/ClusterSimData_SplitHalfTest_2020_11_03_20_10_54.Rdata")
ClusterSimSH = OutputData
rm(OutputData)

ClusterSimSH = lapply(names(ClusterSimSH), function(x)
                        lapply(1:length(ClusterSimSH[[1]]), function(y)
                          lapply(1:length(ClusterSimSH[[1]][[1]]), function(z)
                            (ClusterSimSH[[x]][[y]][[z]]$CC_pvalues)
                          )))
ClusterSimSH = reshape2::melt(ClusterSimSH)
  ClusterSimSH = ClusterSimSH[,-1]
  colnames(ClusterSimSH) =  c("CanonicalCorrelation","value","rep","SampleSize","Dataset")
  ClusterSimSH$SampleSize = as.factor(dplyr::recode(ClusterSimSH$SampleSize, `1`=300, `2`=5000,`3`=10000))
  ClusterSimSH$Sig = as.numeric(ClusterSimSH$value<0.05)
  
  # mean(ClusterSimSH[ClusterSimSH$CanonicalCorrelation==1 & ClusterSimSH$Dataset==1 & ClusterSimSH$SampleSize==300,"Sig"])

  
ClusterSimSH_Avgd = ClusterSimSH %>% 
                        group_by(CanonicalCorrelation,SampleSize,Dataset) %>% 
                         summarise(AvgP = mean(value), SigRate=mean(as.numeric(value<0.05)) )


p1=
ggplot(ClusterSimSH_Avgd[ClusterSimSH_Avgd$Dataset==1,], aes(x=CanonicalCorrelation,y=SigRate, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = 0.05, linetype="dashed") + labs(title="Dataset 1 (r=0.5)",y="Proportion Significant (100 repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,1))

p2=
ggplot(ClusterSimSH_Avgd[ClusterSimSH_Avgd$Dataset==2,], aes(x=CanonicalCorrelation,y=SigRate, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = 0.05, linetype="dashed") + labs(title="Dataset 2 (r=0.3)",y="Proportion Significant (100 repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,1))

p3=
ggplot(ClusterSimSH_Avgd[ClusterSimSH_Avgd$Dataset==3,], aes(x=CanonicalCorrelation,y=SigRate, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = 0.05, linetype="dashed") + labs(title="Dataset 3 (r=0.1)",y="Proportion Significant (100 repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,1))

p4=
ggplot(ClusterSimSH_Avgd[ClusterSimSH_Avgd$Dataset==4,], aes(x=CanonicalCorrelation,y=SigRate, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = 0.05, linetype="dashed") + labs(title="Dataset 4 (r=0)",y="Proportion Significant (100 repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,1))

p1 + p2 + p3 + p4 + plot_layout(ncol=2, byrow = TRUE) + plot_annotation(title = 'Figure S4', subtitle = "Percentage of significant canonical correlations when drawing repeated (XX repeated) samples from simulated data.")


  
```

The split half approach produces sensible p-values, with some caveats. 

Where there is no relationship between predictors and outcome variables (Dataset 4, 0 canonical correlation), the false-positive error rate is controlled at the chosen alpha level (5%).

Where the first canonical correlation is weak (r = .1, Dataset 3), or the sample size is small (N=300), the proposed approach have elevated false-positive error rates for the 2nd and 3rd canonical correlations. 

Therefore, with small samples users should be cautious about drawing conclusions about the number of "true" canonical correlations in the population. 

```{r, fig.height=9, fig.width=10, echo = FALSE }
load("Output R Data/ClusterSimData_SplitHalfTest_2020_11_03_20_10_54.Rdata")
ClusterSimSH = OutputData
rm(OutputData,ClusterSim1,ClusterSim1_BCV2)

ClusterSimSH = lapply(names(ClusterSimSH), function(x)
                        lapply(1:length(ClusterSimSH[[1]]), function(y)
                          lapply(1:length(ClusterSimSH[[1]][[1]]), function(z)
                            as.vector(ClusterSimSH[[x]][[y]][[z]]$CanonicalCorrelations)
                          )))
ClusterSimSH = reshape2::melt(ClusterSimSH)
  colnames(ClusterSimSH) =  c("value","rep","SampleSize","Dataset")
  ClusterSimSH$CanonicalCorrelation = rep(1:5,5000)
  ClusterSimSH$SampleSize = as.factor(dplyr::recode(ClusterSimSH$SampleSize, `1`=300, `2`=5000,`3`=10000))

  
ClusterSimSH_Avgd = ClusterSimSH %>% 
                        group_by(CanonicalCorrelation,SampleSize,Dataset) %>% 
                         summarise(MeanR2 = mean(value^2) )


p1=
ggplot(ClusterSimSH_Avgd[ClusterSimSH_Avgd$Dataset==1,], aes(x=CanonicalCorrelation,y=MeanR2, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = c(.5^2), linetype="dashed") + geom_hline(yintercept = c(0.472649666^2), linetype="dashed", col="grey") + 
  labs(title="Dataset 1 (r=0.5)",y="Average R2 (100 Repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,.25))

p2=
ggplot(ClusterSimSH_Avgd[ClusterSimSH_Avgd$Dataset==2,], aes(x=CanonicalCorrelation,y=MeanR2, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = .3^2, linetype="dashed") + geom_hline(yintercept = c(0.276494574^2), linetype="dashed", col="grey") +
  labs(title="Dataset 2 (r=0.3)",y="Average R2 (100 Repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,.25))

p3=
ggplot(ClusterSimSH_Avgd[ClusterSimSH_Avgd$Dataset==3,], aes(x=CanonicalCorrelation,y=MeanR2, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = .1^2, linetype="dashed") + geom_hline(yintercept = c(0.092387498^2), linetype="dashed", col="grey") +
  labs(title="Dataset 3 (r=0.1)",y="Average R2 (100 Repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,.25))

p4=
ggplot(ClusterSimSH_Avgd[ClusterSimSH_Avgd$Dataset==4,], aes(x=CanonicalCorrelation,y=MeanR2, group=SampleSize, col=SampleSize)) + geom_line() + 
  jtools::theme_apa() + geom_hline(yintercept = 0, linetype="dashed") +
  labs(title="Dataset 4 (r=0)",y="Average R2 (100 Repeats)", x="Canonical Correlation", col="Sample Size") + theme(legend.title =  element_text(color = "black")) + ylim(c(0,.25))

p1 + p2 + p3 + p4 + plot_layout(ncol=2, byrow = TRUE) + plot_annotation(title = 'Figure S2', subtitle = 'Average Canonical Correlation Strength ${R^2}$ from repeated sampling of the simulated datasets. Datasets were simulated with only a single canonical correlation of varying strength.')



  
```

Similar to the first approach (Figure S2), the average squared canonical correlations estimated using this approach yields sensible results. 

However, we find that when the sample is small (N=300), the average squared canonical correlations are very low. This is good for the 2-5 canonical correlations which should be near 0, but the average squared canonical correlation is very low even when the population canonical correlation is high (see top left).


